% Brett
\chapter{Experience with Kokkos}
\section{Performance}
%Comparing Kokkos to Cuda and OpenMP
%Doing this for sanity check
%How we did this (used same algorithm and ran both, timed)
%some graphical results
%analysis of the results and differences
%conclusion that they are very similar performing

Since Kokkos uses Cuda and OpenMP as a backend to achieve faster performance, we
chose to do some testing to confirm that Kokkos performs as well as these two
solutions. If Kokkos performed worse then Cuda or OpenMP, then programmers might
prefer these other solutions instead.  Fortunately, we found that Kokkos matches
the performance of Cuda and OpenMP in almost all cases.  The rest of this
section will describe our strategy for testing the performance of Kokkos
compared to Cuda and OpenMP, present graphs showing the differences observed,
and analyze the graphs.

In order to compare Kokkos, Cuda, and OpenMP, we wrote algorithmically
equivalent code using all three paradigms (using the same data layout and memory
access pattern) and recorded the runtime of each version.  To reduce noise in
the timing data, we repeated the same calculations five times and used the
average time.

Note that because we were unsure how Kokkos implements the team\_reduce()
function, we could not write a matching Cuda reduction.  Based on our project
priorities, we chose not to pursue this further.

These graphs present some of the performance differences and similarities of
Kokkos, Cuda, and OpenMP. Figure~\ref{fig:ContractDataDataScalar Kokkos
performance comparison} shows the raw times of Kokkos Cuda, Cuda, Kokkos OpenMP,
and OpenMP for ContractDataDataScalar.

\begin{figure}[!ht]
{\includegraphics[scale=.4]{CDDS_RawTimes_2d_largestSize_Comparison.pdf}}
\caption[ContractDataDataScalar Kokkos performance comparison]{
    Performance of Kokkos Cuda, Cuda, Kokkos OpenMP,
and OpenMP for ContractDataDataScalar with a memory size of 1 GB.}
\label{fig:ContractDataDataScalar Kokkos performance comparison}
\end{figure}

In this graph, the y-axis is time in seconds, so closer to zero is better. The
x-axis plots different contraction sizes.  Here, Kokkos OpenMP and OpenMP are
almost perfectly overlapping. We are not quite sure why they are not perfectly
overlapping, but it appears too consistent to be random noise. However, the
difference is small enough as to be fairly insignificant. 

Kokkos Cuda, however, shows major differences compared to Cuda. The two perform
identically for the smaller problems but diverge by a significant amount for
bigger problems. This trend exists because Kokkos launches a different number of
blocks compared Cuda; Kokkos launches fewer blocks, with the intention of
reusing them.  We believe this doesn't affect small problem sizes because such
problems require fewer blocks than the large problem sizes, so both Kokkos and
Cuda launch enough blocks.  However, there is clearly a difference in the bigger
problem sizes. 

Figure~\ref{fig:cffscomparison} shows ContractFieldFieldScalar with the slicing
technique (which uses shared memory) for both Kokkos Cuda and Cuda.  It also
includes the flat parallel algorithm in both Kokkos Cuda and Cuda.

\begin{figure}[!ht]
{\includegraphics[scale=.4]{CFFS_RawTimes_2d_largest_Comparison.pdf}}
\caption[ContractFieldFieldScalar Kokkos performance comparison]{
    Performance of the ``slicing'' nested parallelism approach.}
\label{fig:cffscomparison}
\end{figure}

In Figure~\ref{fig:cffscomparison}, the Cuda slicing performance is almost
identical to the Kokkos slicing performance. This shows that Kokkos's use of
shared memory matches that of Cuda.

Overall, most of our graphs show that Kokkos performs almost identically to Cuda
and to OpenMP.  We therefore accepted that Kokkos is not adding any major
overhead. There may be slight differences due to optimization choices, but
Kokkos performs similarly to the other multithreading solutions.

\section{Code Snippets}
%Amount of work the programmer has to do is important
%control and complexity of code is important
%Much more complex than OpenMP, but follows a different paradigm
%Very similar to Cuda, except kernel is a functor
%Overall a pretty easy language, some magic values but pretty good
Another major factor that plays into whether or not a programmer uses a certain
language, feature, library, etcetera, is code complexity and ease of coding.
Although this can be subjective, there are a couple differences between the
languages we would like to point out, especially regarding amount of code
required compared to Cuda or OpenMP (although comparing to OpenMP may be unfair)
and intuitiveness of the code (or readability). 

Regarding the amount of code required, comparing Kokkos to OpenMP does not seem
fair. Comparing OpenMP to almost anything seems unfair, because OpenMP requires
very little code and most of the work is done for you. Although, OpenMP only
works on the CPU which is why it does not require the extra code that Kokkos
requires. In general, OpenMP cannot be compared to Kokkos. If the programmer
knows the code only needs to be multithreaded on the CPU and will never need
more threads then we would strongly advise them to use OpenMP because it is very
simple. However, that is not the niche that Kokkos is trying to fill. 

Kokkos compared to Cuda however, requires similar amounts of code. 
Throughout the code comparison of Cuda and Kokkos we will show
code snippets and point out the differences and similarities directly. We will
start by showing the data setup, because the data needs to get onto the GPU
somehow, then we will move to compare and contrast the Cuda kernel and the Kokkos
functor. \\
\\
Look at Figure~\ref{lst:ContractFieldFieldScalar Cuda Data Setup} that shows the setup of the data on the GPU for Cuda. \\
\begin{figure}[!htb]
	\begin{lstlisting}
float * dev_leftDataArray;
checkCudaError(cudaMalloc((void **) &dev_leftDataArray, 
	numContractions * numLeftFields * numPoints * 
	sizeof(float)));
	
checkCudaError(cudaMemcpy(dev_leftDataArray, &leftDataArray[0], 
	numContractions * numLeftFields * numPoints * sizeof(float), 
	cudaMemcpyHostToDevice));
	\end{lstlisting}
\caption{Code from Cuda \texttt{ContractFieldFieldScalar}
\label{lst:ContractFieldFieldScalar Cuda Data Setup}}
\end{figure}
\\
There are essentially three steps in the process: declaring a pointer to the
data on the CPU, creating an array with the correct size on the GPU, then
copying the data over to the GPU from wherever the data is currently kept on the
CPU. This process is pretty simple and self-explanatory. Now let us compare that
to Kokkos code by looking at Figure~\ref{lst:ContractFieldFieldScalar Kokkos Cuda Data Setup} \\
\begin{figure}[!htb]
	\begin{lstlisting}
typedef Kokkos::Cuda	DeviceType;
typedef Kokkos::View<float***, Kokkos::LayoutRight, DeviceType>
	ContractionData;
typedef typename ContractionData::HostMirror
	ContractionData_Host;

ContractionData dev_ContractData_Left("left_data",
	numContractions,
	numLeftFields,
	numPoints);

ContractionData_Host contractionData_Left = 
	Kokkos::create_mirror_view(dev_ContractData_Left);

for (int cell = 0; cell < numContractions; ++cell) {
	for (int lbf = 0; lbf < numLeftFields; ++lbf) {
		for (int qp = 0; qp < numLeftFields; ++qp) {
			contractionData_Left(cell, lbf, qp) = 
				contractionDataLeft[cell*numLeftFields*
				numPoints + lbf*numLeftFields + qp];
		}
	}
}
	\end{lstlisting}
\caption{Code from Kokkos Cuda \texttt{ContractFieldFieldScalar}
\label{lst:ContractFieldFieldScalar Kokkos Cuda Data Setup}}
\end{figure}
\\
The Kokkos code first defines and creates the device and host Views. One of the
major differences compared to Cuda is that Kokkos uses its own data structure,
a View, instead of an array. This is why we need to use typedefs to define the
Views, but the extra work (which honestly is not much of a hassle) allows the
programmer much more control over the data. The control also comes at the cost
of having to use for loops to copy the data into the host view instead of being
able to do a Memcpy. However, this is all initial work that needs to be done
once, while the benefit of being able to change the layout of the data by
changing the Kokkos::LayoutRight to Kokkos::LayoutLeft is very useful,
especially since this allows the programmer to optimize data layout for both
the CPU and GPU. Overall, Kokkos' and Cuda's data setup have different
philosophies, which makes sense because Kokkos needs to be easily optimized for
both the CPU and GPU while Cuda only runs on the GPU. 

Looking at the "guts" of the programs, Cuda has a kernel that is launched where
all the computation is, while Kokkos uses a functor, almost identical to
Intel's Thread Building Blocks threading paradigm. However, for programs doing
the same calculation, the parenthesis operator function in Kokkos' functor is
almost an exact replica of the code in Cuda's kernel. Look at the code for a Cuda kernel
for ContractFieldFieldScalar in Figure~\ref{lst:ContractFieldFieldScalar Cuda kernel}: \\
\begin{figure}[htb]
	\begin{lstlisting}
__global__ void
cudaContractFieldFieldScalar_Flat_kernel(int numContractions,
	int numLeftFields,
	int numRightFields,
	int numPoints,
	float * __restrict__ dev_contractData_Left,
	float * __restrict__ dev_contractData_Right,
	float * dev_contractResults) {
	int contractionIndex = blockId.x * blockDim.x + threadIdx.x;
	while (contractionIndex < numContractions) {
		int myID = contractionIndex;
		int myCell = myID / (numLeftFields * numRightFields);
		int matrixIndex = myID % (numLeftFields * 
			numRightFields);
		int matrixRow = matrixIndex / numRightFields;
		int matrixCol = matrixIndex % numRightFields;
		
		// Calculate now to save computation later
		int lCell = myMatrix * numLeftFields * numPoints;
		int rCell = myMatrix * numRightFields * numPoints;
		int resultCell = myMatrix * numLeftFields * 
			numRightFields;
		
		float temp = 0;
		for (int qp =0; qp < contractionSize; qp++) {
			temp += dev_contractData_Left[lCell + 
				qp*numLeftFields + matrixRow] *
				dev_contractData_Right[rCell + 
				qp*numRightFields + matrixCol];
		}

		dev_contractResults[resultCell + 
			matrixRow * numRightFields + matrixCol] = 
				temp;
		
		contractionIndex += blockDim.x * gridDim.x;
	}
}
	
	\end{lstlisting}
\caption{Code from Cuda \texttt{ContractFieldFieldScalar}
\label{lst:ContractFieldFieldScalar Cuda kernel}}
\end{figure} \\
\\
Now compare this to the parenthesis operator code for the Kokkos functor in
Figure~\ref{lst:ContractFieldFieldScalar Kokkos Cuda functor}: \\
\begin{figure}[htb]
	\begin{lstlisting}
KOKKOS_INLINE_FUNCTION
void operator() (const unsigned int elementIndex) const {
	int myID = elementIndex;
	int myCell = myID / (_numLeftFields * _numRightFields);
	int matrixIndex = myID % (_numLeftFields * _numRightFields);
	int matrixRow = matrixIndex / _numRightFields;
	int matrixCol = matrixIndex % _numRightFields;

	float temp = 0;
	for (int qp = 0; qp < _numPoints; qp++) {
		temp += _leftFields(myCell, qp, matrixRow) *
			_rightFields(myCell, qp, matrixCol);
	}
	_outputFields(myCell, matrixRow, matrixCol) = temp;
}
	\end{lstlisting}
\caption{Code from Kokkos Cuda \texttt{ContractFieldFieldScalar}
\label{lst:ContractFieldFieldScalar Kokkos Cuda functor}}
\end{figure}
\\
Although there is some more code for the Kokkos functor (the code required to
declare the data members and the constructor), the Kokkos code looks a lot less
cluttered. The Kokkos functor does not need to deal with figuring out the
thread's ID, because it is an integer given as input, while the Cuda kernel
needs to use blockId.x, blockDim.x, etc. Also indexing into the View is easier,
especially when changing the layout of the data from LayoutLeft to LayoutRight
(or vice versa) because no code changes need to occur in the functor.



\section{Personal Experience and Thoughts}
% Positive experience
% We don't know how to install kokkos and took a while to get things to compile
% Magic word hassles 
% Following examples we can figure out what to do by copying syntax
% No documentation when we wanted to figure out what exactly was happening
% 
A task of the project was to document our experiences and thoughts about Kokkos, including any issues that we have run into. Using new tools and learning new syntax always has its tough periods, and getting used to Kokkos definitely had some periods where we had no idea why a program was not compile or giving an incorrect answer (especially in the beginning). But, after the initial learning curve everything seemed to flow pretty well and make sense. 

Our team has never actually been responsible for installing Kokkos on our machine, instead our liaison, Dr. Carter Edwards, did that for us. We are unable to talk about the difficulties of downloading and installing the Kokkos library on our machine, but we did have lots of trouble trying to compile and linking against Kokkos originally. This was due to the fact that the same flags need to be used when installing and compiling and linking against Kokkos. However, since we did not install Kokkos ourselves and the documentation showing how to compile and link against Kokkos used different flags than what were used during our installation, we struggled for a while. Already this shows how Kokkos' documentation is not as developed as one would like, which we will bring up later, but it is understandable since Kokkos is new. 

Another obstacle that slowed us down when first using, is Kokkos' use of magic
words. For example, Kokkos requires the programmer to typedef Kokkos::Cuda or
Kokkos::OpenMP to device\_type, and it must be device\_type, not some other
name. Although the programmer can easily fix this, if the programmer is unaware
of this requirement it can cause a lot of hassle for a while. Every team member
ran into this at one time or another, but after a while we got used to it. When
following examples we learned to use the same names for the typedefs to make
sure that we did not run into another bug with the same nature. Once again
documentation would have helped in this situation, but there is not much
documentation, all we have are examples. On the bright side however, since we
were able to write all of our programs by simply following a few examples we
were able to see some of Kokkos' intuitiveness. Overall we really enjoy Kokkos'
philosophy and structure, which as mentioned before, is almost identical to
Intel's Thread Building Blocks (TBB). If you are familiar with TBB then
learning Kokkos is almost as simple as learning the syntax because they are in
the same paradigm. 

As previously mentioned, Kokkos has very little documentation. For any emerging
technology it is understandable that the creators choose to focus on
functionality instead of documentation, but the documentation needs to catch up
at some point. The examples were very helpful in getting us to our end goal of
working code, but examples are not as helpful in understanding what exactly is
happening, the meaning behind some portions of code, or why certain code is
necessary. Documentation would have also been helpful in seeing the default
values for functions and Views, as well as the other arguments that could have
been passed instead. There were many times we tried to use Google to find
information about Kokkos, but many times the information would point to
uncommented pieces of code, which is not always helpful in determining what is
going on. Overall we believe the documentation for Kokkos needs to improve in
order for new users to get past the initial learning curve and spread the word
about Kokkos. 

As a whole, our team's experience with Kokkos has been positive and see that it
offers a great alternative to other solutions that allow multithreading on
multiple architectures. A quick overview of the benefits of using Kokkos are:
Kokkos can create multithreaded code on the CPU, GPU, and XeonPhi, Views can
easily change the layout of the data, functors seem to keep the code cleaner
and more readable than Cuda's kernels, and the fact that Kokkos is a C++
library and not a a new language adds simplicity. Some of the downsides and
changes that we believe would improve Kokkos include Views having more layouts
than LayoutRight and LayoutLeft, the use of magic words (or lack of using the
right magic words) can create bugs that are hard to find, the example code
should include comments to describe what is happening, and finally the
documentation needs to improve. However, extended use of Kokkos will solve most
of these problems except for Views being limited to two layout types, which is
why our team had an overall good experience with Kokkos.

%\begin{table}[ht]
%      \begin{tabular} {| l | l | l | l |l |}
%            \hline
%            & \textbf{Scalar} & \textbf{Vector} & \textbf{Matrix} \\
%            \hline
%			\textbf{1} & $A_{i}\text{ } x \text{ }B_{i} = C$ & $A_{\text{i, j}} \text{ }x \text{ }B_{j} = C_i$  & $A_{\text{i, %j}} \text{ }x \text{ }B_{\text{j, k}} = C_{\text{i, k}}$ \\
%			\hline
%			\textbf{2} & $A_{\text{i, j}} \text{ }x\text{ } B_{\text{i, j}} = C$ & $A_{\text{i, j, k}}\text{ } x\text{ } B_{\text{j, %k}} = C_i$ & $A_{\text{i, j, k}} \text{ }x\text{ } B_{\text{h, j, k}} = C_{\text{i, h}}$ \\
%			\hline
%			\textbf{3} & $A_{\text{i, j, k}} \text{ }x\text{ } B_{\text{i, j, k}} = C$ & $A_{\text{i, j, k, l}}\text{ } x\text{ } %B_{\text{j, k, l}} = C_i$ & $A_{\text{i, j, k, l}} \text{ }x\text{ } B_{\text{h, j, k, l}} = C_{\text{i, h}}$ \\
%            \hline
%        \end{tabular}
%\caption{Summary of the nine Intrepid tensor contraction kernels
%\label{tab:stuff}}
%\end{table}




