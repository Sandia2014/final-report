\chapter{Parallelism}

% Brett
\section{Flat Parallelism}
When the project started, the version of Kokkos installed on our machine did not include teams. That meant that we could not use any algorithms that required shared memory or reductions. We had access to an atomic fetch and add function, but this can cause a huge bottleneck in programs if too many threads are trying to write to the same memory location. This meant we were limited to writing algorithms in which each thread knew its responsibility and did not have to worry about race conditions, in other words it did not interact with other threads in any capacity. In this section we will describe how to write high performing kernels for both the CPU and GPU using this flat parallelism technique. We will also describe some of its shortcomings and which other non-flat parallel algorithms can help fix these shortcomings.

The main factors that greatly effect performance are the thread count, a thread's responsibility, access patterns, and how data is laid out in memory. The first two of which are all linked together and directly effect each other. Figuring out what the thread count for a given problem should be is typically done by breaking down the problem into smaller pieces and calculating how many smaller pieces there are. For example, in ContractFieldFieldScalar, which is many matrix multiplications, it is easy to make one thread do one matrix multiplication meaning the thread count must be equal to the number of cells, or matrix multiplications, that must be calculated. Figuring out the best way to break down the responsibility of a single thread requires looking at the expected problem size. Since the average use case for the example of ContractFieldFieldScalar is one thousand to tens of thousands of cells, with matrix sizes around 
eight by eight or sixty-four by one hundred twenty-five, it is best to break down the problem into the smallest possible pieces. This is because creating a thread for every cell is ill-advised because that means as little as one thousand threads may be created, which is nowhere near saturating the GPU, which means we want many threads to calculate a single matrix multiplication. Since the one thousand to a couple tens of thousands of cells is the expected use case for all of the contraction problems, the idea of wanting many threads per contraction holds true for all of them. 

The question now is: how many threads per contraction is optimal? Well, looking at our thread count, which is going to be a couple thousands multiplied by the number of threads per contraction, it is best to have anywhere between a couple hundred to a thousand. This will ensure that the GPU is being saturated and the most parallelism is taking place, meaning higher performance. However, the limitation of the threads not being able to interact or write to the same memory location means the most threads per contraction is equal to the number of entries in the output tensor. Looking at our problem size again, the expected output tensor dimensions range from a single number (for any problem that contains DataData in its name), to sixty-four squared (biggest values for numLeftFields and numRightFields which are the dimensions of the output matrix for some problems). This range is smaller than the couple hundred to a thousand that we were hoping for, which means it is best to create a thread per output element per contraction. The thread count and it's responsibility are now known and optimized for our problem sizes. The logic may vary depending on problem size and dimensions, but the goal of saturating the GPU is always the priority.

Now that the number of threads and their responsibility are known, the best access patterns and data layout must be calculated. As described earlier, the best access patterns and data layouts for threads on the CPU are ones that utilize the cache. This means that for the CPU, or when using Kokkos::OpenMP, we want a thread's next memory read to be next to its current memory read. However, on the GPU, or when using Kokkos::Cuda, a thread's memory read should be next to the memory read of the thread right next to it. These two optimizations directly conflict, but lucky for us, Kokkos' View data structure has two layouts, LayoutLeft and LayoutRight, which changes how data is laid out in memory. This means we can use LayoutLeft for one architecture, say Kokkos::Cuda, and LayoutRight for the other, Kokkos::OpenMP. So the trick is figuring out how to arrange the data, or which order to put the indices so that one layout coalesces the memory while the other uses the cache. This is best shown by an example, in ContractFieldFieldScalar there are inputs $A_{c, l, p}$ and $B_{c, r, p}$. Assuming we have a thread per output element in output $C_{c, l, r}$, then we can have the inputs ordered as follows: $A_{c, l, p}$ and $B_{c, r, p}$. When using Kokkos::OpenMP, the Views will be LayoutRight, which means incrementing the rightmost index (in this case $p$ for both inputs $A$ and $B$) will be next to the current data point in memory. So $A_{i, j, k}$ will be right next to $A_{i, j, k+1}$ in memory, while $A_{i, j, k}$ will be very far from $A_{i+1, j, k}$ in memory. Each thread needs to do a dot product of a row in $A$ with a column in $B$, so a thread needs to loop through all of $p$ for the same value of $c$ and $l$ in $A$ and also loop through all of $p$ for the same $c$ and $r$ in $B$. Notice however that all the different value of $p$ in $A$ and $B$, where $c$, $l$, and $r$ are fixed next to each other in memory. This means that it will be cache friendly for any thread. 

Now looking at Kokkos::Cuda with LayoutLeft, memory coalescing depends on what a thread's responsibility is relative the thread before it, meaning to get the best performance the algorithm needs to be smart about which thread does which work. Since $c$ values that differ by 1 are next to each other in memory (because LayoutLeft is being used now), it is best to have thread $x+1$ to do the same work as thread $x$, but for the next cell. For example, thread $x$ is responsible for calculating $C_{i, j, k}$ so thread $x+1$ is responsible for calculating $C_{i+1, j, k}$, this way their memory is always coalesced. 

Finding the best way to layout memory to optimize for both the CPU and GPU is not too difficult. It mainly relies on first finding the best way for caching, then imagine using the opposite layout (left or right) and seeing if there is any way to coalesce the memory. This way one functor can be used, the data layout easily changed, and the performance for both the CPU and GPU will be high. Using this technique we have received many good results, but there are some issues with flat parallelism. First, here are some of examples of the good results:

TODO: Put in graphs of the good speedup times!

The biggest issue with the flat parallelism technique is that there isn't always enough parallelism for the GPU. For all the problems that have DataData in the name, the output is simply an array of numbers, meaning that only one thread per contraction was used. The GPU is great for using tens of thousands of threads to do computation, but when the number of threads is severely limited, the CPU performs much better. For some of our problems, there are test runs where serial CPU code out performed the same problem running on the GPU. This is a scenario that should not happen for these types of problems. The issue is that a small number of threads were created, but there was a lot of work that the thread needed to do. Since CPU's are much faster when comparing a single thread to a single thread on the GPU, it is no surprise the CPU outperforms the GPU in this situation. However to fix it, a threads job needs to be split up between more threads, but this is not allowed in flat parallelism because they threads would need to a mechanism to avoid race conditions when writing to the same location. This is where the reduction method that is described in the next section becomes useful.

So far we have described how to optimize performance using flat parallelism, but there are other algorithms that perform even better than flat parallelism. These algorithms, which are talked about in the next couple sections, make use of shared memory, which is essentially a user controlled cache on the GPU. However, the reason flat parallelism is useful is because it is less complex since it does not make use of shared memory, easier to code, and creates modest performance boosts on its own. 

% Ellen
\section{Reduction}
As shown in Figure %FIXME ref relevant graph in the flat parallelism section
in some cases, flat parallelism can perform very poorly.  One problem with flat
parallelism is the potential lack of enough parallelism, as in
\texttt{ContractDataDataTensor}.  \texttt{ContractDataDataTensor} takes two
input arrays of three-dimensional tensors and produces an array of scalars.  See
Section~\ref{section:ContractDataDataTensor} for a more complete description of
this kernel.

The problem with the \texttt{DataData} class of tensor contractions (see
Table~\ref{tab:IntrepidNamingConvention} is that they all output an array of
scalars -- that is, each individual contraction produces a scalar output.
Therefore, using flat parallelism, the greatest number of threads we can spawn
is one thread per contraction.  Each thread must then perform an entire
contraction independently, which in the case of \texttt{ContractDataDataTensor},
means looping over all three of the contraction indices.

Because of this, we see that when the contraction size is large and the memory size is
small -- when we cannot spawn enough threads to saturate the GPU and each thread
is responsible for a large amount of computation --
\texttt{ContractDataDataTensor} actually performs worse than serial.

A solution to this problem is to use a parallel reduction algorithm instead of a
flat parallelism algorithm.  In a reduction, multiple threads contribute to a
single output element.  This adds the necessary overhead of coordinating between
threads and combining their contributions.

In Kokkos, threads can be organized into teams, which correspond to Cuda blocks.
Built-in reduction methods allow teams to merge the contributions of its
constituent threads.

Using this team-thread paradigm, we explored several methods of implementing
\texttt{ContractDataDataTensor} using a reduction algorithm.

\subsection{Team Depth 1}
    %TODO
\subsection{Team Depth 2}
    %TODO
\subsection{Teamstride}
    %TODO

% Alex
\section{Slicing}
Another general method we used for these contractions was Slicing. The first step of this method was to load one full contraction from the left matrix into shared memory. Then, we simultaneously computed every output element that was dependent on that contraction as input. The clearest way to explain the algorithm is by example. Consider one of the matrix multiplications in ContractFieldFieldScalar. 

\begin{figure}
    \centering
    \includegraphics[scale = .55]{ContractFieldFieldScalarGraphic}
    \caption{Demonstration of memory accesses for a slicing implementation of ContractFieldFieldScalar}
\end{figure}

	On the left, we have the first of the two input matrixes, who's first row elements are labeled $A-E$. On the right we have the second of the two inputs. For the sake of simplicity, assume that we have on block of five threads which are labeled by color. Each of the threads reads in one of the elements on the right and copies it into shared memory. In cases where the number of elements per contraction (row on the left) is unequal to the number of contractions (columns on the right), we set the number of threads per block equal to the number of contractions. This causes threads to either sit idle or loop multiple times when reading the elements on the left into shared memory, but this is clearly more efficient than forcing threads to compute more than one element.
	
	After the values of the contraction have been read into shared memory, we have each thread compute the output element corresponding to one contraction. This is shown on the right by the colored columns. Each thread reads every element from shared memory and computes the contraction by multiplying these elements with the columns of the right matrix. We see that throughout this progress, memory accesses will be coalesced within the block, since each thread reads the same element from shared memory then multiplies by an element that is adjacent to the other elements the rest of the block is reading at that time. 
	
	For every other block of threads, the approach is similar, if Figure 3.1 represents the first block of the contraction, then the second block will be represented as below. 

\begin{figure}[b]
    \centering
    \includegraphics[scale = .55]{ContractFieldFieldScalarGraphic2}
    \caption{Demonstration of memory accesses for the second block of a slicing implementation of ContractFieldFieldScalar}
\end{figure}

We see that for the FieldFieldScalar example, where our equation is given by $L_{C,\ell,P} \times R_{C, \mathcal{R}, P} = O_{C,\ell, \mathcal{R}}$, the number of blocks initialized by the algorithm will be equal $\ell \times C$, since there are $\ell$ blocks per matrix, and we have $C$ matrices. Additionally, there will be $\mathcal{R}$ threads per block. 

	Code for executing the algorithm as described above is included below, although it has been simplified for clarity. 

\begin{figure}[ht]
    \begin{lstlisting} [basicstyle=\tiny]
     extern __shared__ float sliceStorage[];
     const unsigned int col = threadIdx.x;
     unsigned int currentBlock = blockIdx.x;
     unsigned int numBlocks = numBasis*numCells;
     
     syncthreads();
     const unsigned int cell = currentBlock / numBasis;
     const unsigned int row = currentBlock - cell * numBasis;

     for (unsigned int p = threadIdx.x; p < contractionSize; p += blockDim.x) {
        sliceStorage[p] = dev_contractionData_Left[cell*numBasis*contractionSize + row*contractionSize + p];
     }
     syncthreads();

     float sum = 0;
     for (int p = 0; p < contractionSize; ++p) {
       sum += sliceStorage[p] * dev_contractionData_Right[cell*numBasis*contractionSize +
        p*numBasis + col];
     }

     dev_contractionResults[cell*numBasis*numBasis + row*numBasis + col] = sum;
 \end{lstlisting}
\caption{Code from slicing algorithm on \texttt{ContractFieldFieldScalar}
\label{lst:ContractFieldFieldScalarSlice}} 
\end{figure}

	The main advantage of this approach is that it is easily generalizable to tensor contractions of higher dimensions. Unlike tiling, which is significantly less intuitive in higher dimensions, it is easy to implement slicing in higher dimensions by loading a larger slice into shared memory. Because of its reliance on shared memory, there are many use cases in which we would expect slicing to perform poorly. Intuitively, slicing is reliant on large contraction sizes to produce speedup because in situations where the number of threads per block is low it is unable to saturate the GPU. While this problem can be remedied by increasing the number of contractions per block, it can introduce problems with shared memory. Since shared memory is limited by nature, slicing has to balance the amount of work per block with the amount of shared memory available to that block.
	
	In situations where the problem has an inherently large amount of reuse like ContractFieldFieldScalar, this problem can be remedied to some degree, but in contractions without this feature, like ContractDataDataScalar, it seems clear that slicing will not be an efficient algorithm. 
	
When we compare slicing approaches using one contraction per block to independent flat parallelism on promising problems, we get underwhelming results. 

    \includegraphics[scale = .17]{slicingvsindependent}

These results were generated by comparing independent algorithms to slicing on ContractFieldFieldScalar with $\ell = \mathcal{R} = 10, P = 8 - 1024$. We see that in the corner where the memory size is small and contraction size is small we get a small amount of speedup relative to independent Cuda code, which is promising. This is the corner where we would expect slicing to perform the best in comparison to independent parallelism, since in this corner flat parallelism is unable to fully saturate the GPU. The benefits of reuse in this corner are significant enough to outcompete flat parallelism. On the rest of the graph, however, the inability of slicing to saturate the GPU means that it is significantly slower that flat parallelism. Since $\ell = \mathcal{R} = 10$, the algorithm naturally only spawns 10 threads per block, which is not enough to produce good results. 

On problems with larger basis functions we see better results for slicing. For example, consider the following use case of ContractFieldFieldTensor: $\ell = \mathcal{R} = 125$, $P = 216$, $t_1 = t_2 = 3$.

\vspace{10mm}
the data exists for this here: https://github.com/Sandia2014/kokkos-intrepid/tree/slicing+tiling/ContractFieldFieldTensor, will make graph later


We see that while slicing performs better, it is still eclipsed by independent cuda. 

We're still working on slicing that does two rows per block so hopefully that will be done soon and we'll have data for it. 

% Alex
\section{Tiling}

The final parallelization technique we used for these tensor contractions was tiling. This technique is similar to the tiled technique for matrix multiplication used in serial operations. Instead of relying on the cache to retain the relevant pieces of information, however, we use shared memory to explicitly store the data we care about. Once again, we will explain this algorithm by example. Consider one of the matrix multiplications in ContractFieldFieldScalar shown above. 

\begin{figure}
    \centering
    \includegraphics[scale = .7]{ContractFieldFieldScalarGraphicTiling}
    \caption{Demonstration of memory accesses for a tiling implementation of ContractFieldFieldScalar}
\end{figure}

For the sake of simplicity we'll consider a block to be four threads, which simplifies our computation since the matrix is four by four. On the left hand side, the block loads a four element tile into the shared memory of the threads. Once these elements are loaded into memory, each thread can begin computation of their element in the output matrix. Each thread computes as much of their output element as they can using the elements in shared memory, then we load a new tile into shared memory and continue the process, as shown below. We see that in this case we will have to load two tiles into shared memory before we have computed every output element in its entirety. 

\begin{figure}
    \centering
    \includegraphics[scale = .7]{ContractFieldFieldScalarGraphicTiling2}
    \caption{Demonstration of memory accesses for a tiling implementation of ContractFieldFieldScalar}
\end{figure}

	Tiling can be viewed as a more specialized version of slicing, since they both use similar access patterns for shared memory. The difference between the two lies in tilings usage of multiple contractions per block, as well as the the distribution of a contractions operations over multiple loops of the routine. Because of these differences, tiling can routinely saturate the GPU in a way that pure slicing cannot, since the algorithm inherently limits the shared memory usage per block by reusing the same shared memory multiple times. Additionally, if we set the dimension of our tiles intelligently, we can reliably saturate the GPU with both blocks and threads, something that is very difficult to do adaptively with pure slicing. 
	
	Unfortunately, it is much less clear how exactly to tile in multiple dimensions. Unlike slicing, there seem to be multiple distinct ways of approaching the problem. One could create "tiles" with dimension equal to the contraction size, or any number less than the contraction dimension by unrolling the contraction to some intermediate degree. We haven't been able to fully explore every possibility in this area, and have simply treated the higher dimensional contractions as a fully unrolled contraction of one dimension. It is possible, however, that in some situations it would be more effective to create tiles with multiple degree. These tiles would have a different layout in memory who's efficiency would vary by situation. 
	
	Excerpts from our Cuda implementation of tiling are included below. The code assumes that tileSize (the horizontal and vertical dimensions of a tile) evenly divides both the contraction size and $\ell = \mathcal{R} = \text{numBasis}$.
	
\begin{figure}
    \begin{lstlisting} [basicstyle=\tiny]
  extern __shared__ float tileStorage[];
  const unsigned int numbersPerTile = tileSize * tileSize;
  const unsigned int numberOfHorizontalTiles = contractionSize / tileSize;
  const unsigned int numberOfVerticalTiles = numBasis / tileSize;

  const unsigned int numberOfTiles = numCells * numberOfVerticalTiles * numberOfVerticalTiles;

  const unsigned int subRow = threadIdx.x / tileSize;
  const unsigned int subCol = threadIdx.x  - subRow * tileSize;

  unsigned int resultTileIndex = blockIdx.x;

  unsigned int resultSubmatrixIndex = resultTileIndex % (numberOfVerticalTiles * numberOfVerticalTiles);
  unsigned int resultMatrix = resultTileIndex / (numberOfVerticalTiles * numberOfVerticalTiles);

  // for tileNumber in 0...numberOfTilesPerSide
  for (unsigned int tileNumber = 0; tileNumber < numberOfHorizontalTiles; ++tileNumber) {
      
      // calculate result tile indices
      const unsigned int resultTileRow = resultSubmatrixIndex / numberOfHorizontalTiles;
      const unsigned int resultTileCol = resultSubmatrixIndex  -
        resultTileRow * numberOfHorizontalTiles;

      // calculate this threads actual output index
      const unsigned int row = resultTileRow * tileSize + subRow;
      const unsigned int col = resultTileCol * tileSize + subCol;

      // these are base indices into the shared memory
      const unsigned int leftBaseIndex = subRow * tileSize;
      const unsigned int rightBaseIndex = numbersPerTile + subCol;

      const unsigned int resultIndex = row * numBasis + col;

      // load the left and right tiles into shared memory
      syncthreads();
      tileStorage[threadIdx.x] = dev_contractionData_Left[resultMatrix * numBasis * contractionSize
        + row * contractionSize + tileNumber * tileSize + subCol];
      tileStorage[threadIdx.x + blockDim.x] = dev_contractionData_Right[resultMatrix * numBasis * contractionSize
        + (tileNumber * tileSize + subRow) * numBasis + col];
      
      // make sure everyone's finished loading their pieces of the tiles
      syncthreads();
      double sum = 0;
      for (unsigned int dummy = 0; dummy < tileSize; ++dummy) {
        sum +=
          tileStorage[leftBaseIndex + dummy] *
          tileStorage[rightBaseIndex + dummy * tileSize];
      }
      dev_contractionResults[resultIndex] += sum;
    }

 \end{lstlisting}
\caption{Code from tiling algorithm on \texttt{ContractFieldFieldScalar}
\label{lst:ContractFieldFieldScalarTiling}} 
\end{figure}


Thus far in our research, we have found tiling to be the most effective algorithm for realizing parallel speedup. 

\begin{figure}
    \centering
\includegraphics[scale = .2]{tilinguc1}
\end{figure}
Consider the graph generated above for the following use case of ContractFieldFieldScalar, $\ell = \mathcal{R} = 125$, $P = 216$. We see that Tiling outperforms both flat parallelism and team reductions across the board. This trend continues for smaller use cases as well, as shown below when $\ell = \mathcal{R} = 8$, $P = 8$.

\begin{figure}[h]
    \centering
\includegraphics[scale = .2]{tilinguc2}
\end{figure}

In general, we have found that 2D tiling is the most effective method for achieving parallel speedup on these kernels. While there may be some potential for exploration of higher dimensionality tiles, it seems doubtful that these layouts will be able to accomplish significantly more speedup. 
