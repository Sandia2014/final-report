% Tyler's piece

% CHAPTER 1 -- Intro to everything
\chapter{Sandia National Laboratories}

\section{Background}

Sandia National Laboratories is a federally funded research and development
center owned and managed by the Sandia Corporation.  The laboratory's primary
focus is the maintenance, management, and development of the United States'
nuclear arsenal.  Sandia also performs research in the fields of supercomputing
and scientific computing, which is where our project originates. 

\section{Kokkos}

 The Trilinos Project, developed and maintained by
Sandia, is a collection of open source libraries intended for use in large-scale
scientific applications.
One of the packages in Trilinos is Kokkos, which is designed to aid portability
and performance in software written for manycore architectures.  

Kokkos attempts to mitigate the issues of architecture dependence 
by allowing programmers to write their
code once, and compile for optimization on a variety of manycore architectures.
This is possible because the library helps manage the allocation of memory across devices for the
programmer. Kokkos
also allows users to write thread scalable software by providing an API for
using fast, non-locking data structures.

However, the Kokkos package is still new and relatively untested.  No
large-scale projects have yet been written with it.  Some small kernels have
been rewritten using Kokkos, but there are still many kernels in Trilinos that
would benefit from increased thread scalable parallelization.

\section{Problem}

The task of this clinic has been to rewrite several kernels, using Kokkos, from libraries within
Trilinos to be efficient and thread-scalable on manycore architectures.  Sandia benefits from this task in two ways, firstly the performance gains of the kernels will make more efficient use of their computers meaning they can calculate more than currently possible. Secondly, the kernels will be a use case for Kokkos. Since Kokkos is a new technology part of this clinic is to provide user feedback about Kokkos. The last task of this project is to apply what we learned from parallelizing the kernels to create a general algorithm that Sandia developers can apply to other kernels to for performance boosts. Sandia is interested in a method for parallelizing kernels because they do not have research different methods themselves but use the knowledge we learned to make even more of their code base perform at a higher level.

The kernels we focused primarily on were in a tensor manipulation library within Intrepid, a sub-package of Trilinos. 
The Intrepid package is a library of kernels designed for use by developers who want to reuse large
parts of their existing code frameworks while gaining access to state of the art
tools for compatible discretizations of partial differential equations. As such, by improving the 
performance of these kernels within Intrepid, we can improve performance
of any number of calculation-heavy simulation libraries that rely on Intrepid to calculate tensor contractions.



\section{CPU vs GPU}
The vast
majority of scientific software produced and used by the national labs relies on
message passing parallelism (MPI) to leverage both inter-node and intra-node
parallelism.  
Until the present, this approach of using MPI across a single node
sacrificed some performance for the ease of a monolithic programming model, and
the performance penalty has not been high enough to motivate the usage of
threads instead of processes.  However, as the exascale push hits the power
wall, interest has been growing in the area of using higher performing and less
power-hungry co-processors on each node, still with message passing across
nodes.  Unfortunately, this means that much existing code will have to be rewritten, as
message passing cannot be used to leverage the parallelism of co-processors such
as Graphical Processing Units (GPUs).

GPUs are characterized by high thread counts, decreased memory per thread, and
relatively small instruction sets when compared to CPUs. These differences lead
to many advantages when it comes to high performance computing. Since GPUs have
a smaller instruction set, they can devote more of their transistors to
arithmetic computation. This means that GPUs are capable of executing
significantly more floating point operations per second (FLOPS). Additionally,
GPUs use less power than CPUs, which makes them appealing for supercomputers,
where power consumption is a major concern.

Well implemented GPU code can yield significant speedup in certain
processing-heavy applications.  However, the ways in which an algorithm must be
optimized to run on a GPU are highly dependent on hardware architecture. Additionally, the language
used to write code that runs on different hardware might actually differ.  Finally, 
highly parallel code may suffer from race conditions caused by unsafe data accesses.
Often, using always safe data structures and memory access patterns can reduce performance, 
but performance is at the heart of what Sandia, and others in the supercomputing industry
are %TODO is/are?
interested in.

One of the biggest differences in optimizing for the GPU instead of the GPU is the way memory should be accessed. On the CPU the best memory access pattern is one that makes the most use of the cache. Since caches pull in chunks of memory, all of which are next to each other, at a time, it is best to ask for data directly next to the data just used on the CPU. However, GPUs work very differently. GPUs run their threads in warps. A warp is 32 threads that run in lock step together, which means all 32 threads run the same instruction at the same time. This makes the best memory access patterns on the GPU ones in which the threads in a warp access data directly next to each other, which is called coalescing the memory. As one would guess, coalesced memory is best on the GPU because like the CPU, blocks of memory are pulled into the small caches of the GPU and it is best if all the data the warp requires are pulled in at once instead of over multiple accesses.

The GPU having warps of size 32 effects more than changing the optimized memory access pattern compared to the CPU. Warps also effect the performance of some if statements. Part of the reason conditionals can greatly decrease performance on the GPU is if two threads in a warp take different branches then they both sections in code must be executed. This forces one thread to essentially waste cycles because it does not need to be executing the code in the branch that another thread in its warp requires. 

%TODO WHAT ELSE GOES HERE?
%\subsection{WHAT ELSE GOES HERE?}
%I don't now how in depth you want me to get into GPU architecture. Here is a list of questions I came up 
%with while writing this section:
%\begin{itemize}
%	\item Who is my audience?
%	\item Should I go more in depth?
%	\item Should I talk about caches, and how that's different from CPU to GPU? (i.e. coalesced memory accesses)
%	\item Should there be code snippets here? It's the introduction, so my feeling was no, but we might need more later %then.
%	\item Should I mention shared memory, threads, warps, etc.?
%\end{itemize}










