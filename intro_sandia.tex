% Tyler's piece

% CHAPTER 1 -- Intro to everything
\chapter{Sandia National Laboratories}

\section{Background}

Sandia National Laboratories is a federally funded research and development
center owned and managed by the Sandia Corporation.  The laboratory's primary
focus is the maintenance, management, and development of the United States'
nuclear arsenal.  Sandia also performs research in the fields of supercomputing
and scientific computing. 


\section{CPU vs GPU}
The vast
majority of scientific software produced and used by the national labs relies on
message passing parallelism (MPI) to leverage both inter-node and intra-node
parallelism.  
Until the present, this approach of using MPI across a single node
sacrificed some performance for the ease of a monolithic programming model, and
the performance penalty has not been high enough to motivate the usage of
threads instead of processes.  However, as the exascale push hits the power
wall, interest has been growing in the area of using higher performing and less
power-hungry co-processors on each node, still with message passing across
nodes.  Unfortunately, this means that much existing code will have to be rewritten, as
message passing cannot be used to leverage the parallelism of co-processors such
as Graphical Processing Units (GPUs).

GPUs are characterized by high thread counts, decreased memory per thread, and
relatively small instruction sets when compared to CPUs. These differences lead
to many advantages when it comes to high performance computing. Since GPUs have
a smaller instruction set, they can devote more of their transistors to
arithmetic computation. This means that GPUs are capable of executing
significantly more floating point operations per second (FLOPS). Additionally,
GPUs use less power than CPUs, which makes them appealing for supercomputers,
where power consumption is a major concern.

Well implemented GPU code can yield significant speedup in certain
processing-heavy applications.  However, the ways in which an algorithm must be
optimized to run on a GPU are highly dependent on hardware architecture. Additionally, the language
used to write code that runs on different hardware might actually differ.  Finally, 
highly parallel code may suffer from race conditions caused by unsafe data accesses.
Often, using always safe data structures and memory access patterns can reduce performance, 
but performance is at the heart of what Sandia, and others in the supercomputing industry
are %TODO is/are?
interested in.

%TODO WHAT ELSE GOES HERE?
\subsection{WHAT ELSE GOES HERE?}
I don't now how in depth you want me to get into GPU architecture. Here is a list of questions I came up 
with while writing this section:
\begin{itemize}
	\item Who is my audience?
	\item Should I go more in depth?
	\item Should I talk about caches, and how that's different from CPU to GPU? (i.e. coalesced memory accesses)
	\item Should there be code snippets here? It's the introduction, so my feeling was no, but we might need more later then.
	\item Should I mention shared memory, threads, warps, etc.?
\end{itemize}


\section{Kokkos}

 The Trilinos Project, developed and maintained by
Sandia, is a collection of open source libraries intended for use in large-scale
scientific applications.
One of the packages in Trilinos is Kokkos, which is designed to aid portability
and performance in software written for manycore architectures.  

Kokkos attempts to mitigate the issues of architecture dependence 
by allowing programmers to write their
code once, and compile for optimization on a variety of manycore architectures.
This is possible because the library helps manage the allocation of memory across devices for the
programmer. Kokkos
also allows users to write thread scalable software by providing an API for
using fast, non-locking data structures.

However, the Kokkos package is still new and relatively untested.  No
large-scale projects have yet been written with it.  Some small kernels have
been rewritten using Kokkos, but there are still many kernels in Trilinos that
would benefit from increased thread scalable parallelization.


\section{Problem}

The goal of this clinic has been to rewrite several kernels, using Kokkos, from libraries within
Trilinos to be efficient and thread-scalable on manycore architectures.  
These redesigned and reimplemented kernels will be integrated into
Trilinos' production code, as both a performance improvement and as a use case
for Kokkos.

We focused primarily on a tensor manipulation library within Intrepid, a sub-package of Trilinos. 
The Intrepid package is a library of kernels designed for use by developers who want to reuse large
parts of their existing code frameworks while gaining access to state of the art
tools for compatible discretizations of partial differential equations. As such, by improving the 
performance of these kernels within Intrepid, we can improve performance
of any number of calculation-heavy simulation libraries that rely on Intrepid to calculate tensor contractions.




